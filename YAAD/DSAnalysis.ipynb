{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wget\n",
    "!pip install PyGithub\n",
    "!pip install openpyxl\n",
    "#!pip install pandas --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import path\n",
    "\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import wget\n",
    "from github import Github\n",
    "import shutil\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn import metrics\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from datetime import datetime \n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# signal processing\n",
    "from scipy import signal\n",
    "from scipy.ndimage import label as sci_label\n",
    "from scipy.stats import zscore\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.integrate import trapz\n",
    "\n",
    "# misc\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Set and clear output directories ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all existing csv files\n",
    "!rm *.csv\n",
    "!rm *.xlsx\n",
    "\n",
    "\n",
    "if path.exists('/kaggle/working/Raw Data') == True:\n",
    "  shutil.rmtree(\"/kaggle/working/Raw Data\")\n",
    "\n",
    "if path.exists('/kaggle/working/Raw Data/Multimodal/ECG/') == False:\n",
    "  os.mkdir('Raw Data')\n",
    "  os.mkdir('Raw Data/Multimodal')\n",
    "  os.mkdir('Raw Data/Multimodal/ECG')\n",
    "  \n",
    "\n",
    "\n",
    "if path.exists('/kaggle/working/Raw Data/Single Modal/ECG') == False:\n",
    "  os.mkdir('Raw Data/Single Modal')\n",
    "  os.mkdir('Raw Data/Single Modal/ECG')\n",
    "\n",
    "!pwd\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve datasets\n",
    "\n",
    "Read xlsx files and convert to csv and transform into datafram for processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stimulus_desc_file = pd.read_excel('../input/young-adults-affective-data-ecg-and-gsr-signals/ECG_GSR_Emotions/Stimulus_Description.xlsx')\n",
    "stimulus_desc_file.to_csv('Stimulus_Description.csv', index = None, header=True)\n",
    "stimulus_desc = pd.read_csv('Stimulus_Description.csv')\n",
    "#stimulus_desc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self_annotation_multimodal_file = pd.read_excel('../input/young-adults-affective-data-ecg-and-gsr-signals/ECG_GSR_Emotions/Self-Annotation Labels/Self-annotation Multimodal_Use.xlsx')\n",
    "self_annotation_multimodal_file.to_csv('Self-annotation Multimodal_Use.csv', index = None, header=True)\n",
    "self_annotation_multimodal = pd.read_csv('Self-annotation Multimodal_Use.csv')\n",
    "self_annotation_multimodal['annotation'] = 'M'\n",
    "self_annotation_multimodal.rename(columns = {'V_Label':'Valence', 'A_Label':'Arousal', 'Four_Labels':'Four_Label'}, inplace = True)\n",
    "#self_annotation_multimodal.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self_annotation_singlemodal_file = pd.read_excel('../input/young-adults-affective-data-ecg-and-gsr-signals/ECG_GSR_Emotions/Self-Annotation Labels/Self-annotation Single Modal_Use.xlsx')\n",
    "self_annotation_singlemodal_file.to_csv('Self-annotation Single Modal_Use.csv', index = None, header=True)\n",
    "self_annotation_singlemodal = pd.read_csv('Self-annotation Single Modal_Use.csv')\n",
    "self_annotation_singlemodal['annotation'] = 'S'\n",
    "self_annotation_singlemodal.rename(columns = {'Male':'Gender', 'Session Id':'Session ID', 'Video Id':'Video ID'}, inplace = True)\n",
    "#self_annotation_singlemodal.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self_annotation_frames = [self_annotation_multimodal, self_annotation_singlemodal]\n",
    "merged_dataframe = pd.concat(self_annotation_frames)\n",
    "#merged_dataframe.head()\n",
    "#merged_dataframe.tail()\n",
    "table_frame = merged_dataframe.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = []\n",
    "cols.append('Raw Data')\n",
    "for col in merged_dataframe.columns:\n",
    "    cols.append(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing\n",
    "\n",
    "Processed data: Collected ECG signals are converted into numerical series data values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_shape = 1000\n",
    "raw_data_arr = []\n",
    "\n",
    "def form_data(data_array = [], annotation = '', data_path = ''):\n",
    "    #######\n",
    "    #\n",
    "    #     If anyone could help me improve this code please do so.\n",
    "    #     Goals was to product the same data format of raw_dataframe DF below.\n",
    "    #\n",
    "    #######\n",
    "    for filename in os.listdir(data_path):\n",
    "            data = np.loadtxt(os.path.join(data_path, filename), delimiter=',')\n",
    "            data = data[0:arr_shape]\n",
    "            filenames = filename.split('ECGdata_')[1]\n",
    "            filenames = filenames.split('.dat')[0]\n",
    "            filenames = filenames.lower()\n",
    "            s = filenames.split('s')[1].split('p');\n",
    "            p = s[1].split('v')\n",
    "            s = s[0]\n",
    "            v = p[1]\n",
    "            p = p[0]\n",
    "            data_row = merged_dataframe.loc[(merged_dataframe['Session ID'] == int(s)) & \n",
    "                             (merged_dataframe['Participant Id'] == int(p)) & \n",
    "                             (merged_dataframe['Video ID'] == int(v)) &\n",
    "                             (merged_dataframe['annotation'] == 'M')]\n",
    "            stim_row = stimulus_desc.loc[(stimulus_desc['Session ID'] == int(s)) & \n",
    "                             (stimulus_desc['Video ID'] == int(v))]\n",
    "            for index, row in data_row.iterrows():\n",
    "              data_array.append([data, \n",
    "                                   row['Participant Id'], row['Session ID'], row['Video ID'],\n",
    "                                   row['Name'], row['Age'], row['Gender'], row['Valence level'],\n",
    "                                   row['Arousal level'], row['Dominance level'], row['Happy'],\n",
    "                                   row['Sad'], row['Fear'], row['Anger'], row['Neutral'],\n",
    "                                   row['Disgust'], row['Surprised'], row['Familiarity Score'],\n",
    "                                   row['Emotion'], row['Valence'], row['Arousal'], row['Four_Label'],\n",
    "                                   row['annotation'],  stim_row['Target Emotion'].iat[0]\n",
    "                                   ])\n",
    "    return data_array\n",
    "\n",
    "    \n",
    "raw_data_arr =  form_data(data_array = raw_data_arr, annotation = 'M', data_path = \"../input/young-adults-affective-data-ecg-and-gsr-signals/ECG_GSR_Emotions/Raw Data/Multimodal/ECG/\")\n",
    "raw_data_arr =  form_data(data_array = raw_data_arr, annotation = 'S', data_path = \"../input/young-adults-affective-data-ecg-and-gsr-signals/ECG_GSR_Emotions/Raw Data/Single Modal/ECG/\")\n",
    "cols.append('Target Emotion')\n",
    "raw_dataframe = pd.DataFrame(raw_data_arr, columns = cols)\n",
    "raw_dataframe.rename(columns = {'Participant Id':'Participant ID', 'annotation':'Modal', 'Four_Label':'Four label'}, inplace = True)\n",
    "raw_dataframe['Familiarity Score'] = raw_dataframe['Familiarity Score'].fillna('Never watched')\n",
    "raw_dataframe = raw_dataframe.replace(np.nan, '', regex=True)\n",
    "#raw_dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data visualization\n",
    "\n",
    "Using Matplotlib, we can visualized the data/signals per emotion into signal waves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_frame = raw_dataframe.copy()\n",
    "#plot_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_frame = plot_frame.drop(['Participant ID', 'Session ID', 'Familiarity Score', 'Age', 'Gender', 'Name'], axis = 1)\n",
    "sad_data = plot_frame.loc[(plot_frame['Emotion'] == 'Sad') & (plot_frame['Target Emotion'] == 'sad')] \n",
    "fear_data = plot_frame.loc[(plot_frame['Emotion'] == 'Fear')  & (plot_frame['Target Emotion'] == 'fear')]\n",
    "happy_data = plot_frame.loc[(plot_frame['Emotion'] == 'Happy') & (plot_frame['Target Emotion'] == 'happy')]\n",
    "anger_data = plot_frame.loc[(plot_frame['Emotion'] == 'Anger') & (plot_frame['Target Emotion'] == 'anger')]\n",
    "neutral_data = plot_frame.loc[(plot_frame['Emotion'] == 'Neutral') & (plot_frame['Target Emotion'] == 'neutral')]\n",
    "mixed_data = plot_frame.loc[(plot_frame['Emotion'] == 'Mixed') & (plot_frame['Target Emotion'] == 'neutral')]\n",
    "disgust_data = plot_frame.loc[(plot_frame['Emotion'] == 'Disgust') & (plot_frame['Target Emotion'] == 'disgust')]\n",
    "surprised_data = plot_frame.loc[(plot_frame['Emotion'] == 'Surprise') & (plot_frame['Target Emotion'] == 'surprise')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_signals(data_arr, title = ''):\n",
    "    plt.clf()\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    for index, row in data_arr.iterrows():\n",
    "        y = row['Raw Data']\n",
    "        plt.plot(y)\n",
    "        #x = np.arange(y.size)\n",
    "        #plt.plot(x, y)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SAD**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_signals(data_arr = sad_data, title = 'ECG Signals: SAD')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FEAR**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_signals(data_arr = fear_data, title = 'ECG Signals: FEAR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HAPPY**: Signals scatters on different levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_signals(data_arr = happy_data, title = 'ECG Signals: HAPPY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANGER**: Signals scatters on different levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_signals(data_arr = anger_data, title = 'ECG Signals: ANGER')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NEUTRAL**: Signals above 0 region are scatterd. Some are on -15 to -20 area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_signals(data_arr = neutral_data, title = 'ECG Signals: NEUTRAL')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MIXED**: Most signals resides on zero level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_signals(data_arr = mixed_data, title = 'ECG Signals: MIXED')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DISGUST** : Signals are on 0-5 region and a lot on -20 level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_signals(data_arr = disgust_data, title = 'ECG Signals: DISGUST')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SURPRISED** : signals are all similar and residing near zero region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_signals(data_arr = surprised_data, title = 'ECG Signals: SURPRISED')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Findings:** Uncertain on possible recognized pattern based on visual representation of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model creation and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_d_frame = plot_frame.copy().drop(['Video ID', 'Valence level', 'Arousal level',\n",
    "#                                        'Dominance level', 'Happy', 'Sad', 'Fear',\n",
    "#                                        'Anger', 'Neutral', 'Disgust', 'Surprised',\n",
    "#                                        'Valence', 'Arousal', 'Four label', 'Modal'\n",
    "#                                       ], axis = 1)\n",
    "train_d_frame = plot_frame.copy().drop(['Video ID', 'Happy', 'Sad', 'Fear',\n",
    "                                        'Anger', 'Neutral', 'Disgust', 'Surprised', 'Four label', 'Modal'\n",
    "                                       ], axis = 1)\n",
    "train_d_frame.rename(columns = {'Raw Data':'feature', 'Emotion':'emotion'}, inplace = True)\n",
    "#train_d_frame.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x = np.array(train_d_frame['feature'].tolist())\n",
    "#y = np.array(train_d_frame['emotion'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sc = StandardScaler()\n",
    "#x = sc.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labelencoder = LabelEncoder()\n",
    "#y = to_categorical(labelencoder.fit_transform(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset into training and testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#num_labels = y.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = Sequential()\n",
    "\n",
    "###first layer\n",
    "#model.add(Dense(1, input_shape = (arr_shape,)))\n",
    "#model.add(Activation('relu'))\n",
    "\n",
    "###second layer\n",
    "#model.add(Dense(2))\n",
    "#model.add(Activation('relu'))\n",
    "\n",
    "###third layer\n",
    "#model.add(Dense(4))\n",
    "#model.add(Activation('relu'))\n",
    "\n",
    "###fourth layer\n",
    "#model.add(Dense(8))\n",
    "#model.add(Activation('relu'))\n",
    "\n",
    "###final layer\n",
    "#model.add(Dense(num_labels))\n",
    "#model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.compile(loss = 'categorical_crossentropy', metrics = ['accuracy'], optimizer = 'adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#num_epochs = 10\n",
    "#num_batch_size = 2\n",
    "\n",
    "#checkpointer = ModelCheckpoint(filepath = './Raw Data/ecg_emotion_recognizer.hdf5', \n",
    "#                               verbose = 1, save_best_only = True)\n",
    "#start = datetime.now()\n",
    "\n",
    "#model.fit(x_train, y_train, batch_size = num_batch_size, epochs = num_epochs,\n",
    "#          validation_data = (x_test, y_test), callbacks = [checkpointer], verbose = 1)\n",
    "\n",
    "#duration = datetime.now() - start\n",
    "#print(\"Training completed in time: \", duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_accuracy = model.evaluate(x_test, y_test, verbose = 0)\n",
    "#print(test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initial Findings**: Model displayed low accuracy. Need to look for possible improvements. Will try feature extraction on raw data signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "#from matplotlib import pyplot as plt\n",
    "import scipy.io as spio\n",
    "#import numpy as np\n",
    "import statistics\n",
    "from scipy.stats import kurtosis\n",
    "from scipy.stats import skew\n",
    "import sys\n",
    "#sys.path.append(\"/home/chandan/python-workspace/\")\n",
    "#import BOCPD as ocpd #import bocpd from another file\n",
    "import cProfile\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### FEATURE DEFINITIONS ###################################\n",
    "\n",
    "def detect_peaks(ecg_signal, threshold=0.3, qrs_filter=None):\n",
    "    '''\n",
    "    Peak detection algorithm using cross corrrelation and threshold \n",
    "    '''\n",
    "    if qrs_filter is None:\n",
    "        # create default qrs filter, which is just a part of the sine function\n",
    "        t = np.linspace(1.5 * np.pi, 3.5 * np.pi, 15)\n",
    "        qrs_filter = np.sin(t)\n",
    "    \n",
    "    # normalize data\n",
    "    ecg_signal = (ecg_signal - ecg_signal.mean()) / ecg_signal.std()\n",
    "\n",
    "    # calculate cross correlation\n",
    "    similarity = np.correlate(ecg_signal, qrs_filter, mode=\"same\")\n",
    "    similarity = similarity / np.max(similarity)\n",
    "\n",
    "    # return peaks (values in ms) using threshold\n",
    "    return ecg_signal[similarity > threshold].index, similarity\n",
    "\n",
    "def group_peaks(p, threshold=5):\n",
    "    '''\n",
    "    The peak detection algorithm finds multiple peaks for each QRS complex. \n",
    "    Here we group collections of peaks that are very near (within threshold) and we take the median index \n",
    "    '''\n",
    "    # initialize output\n",
    "    output = np.empty(0)\n",
    "\n",
    "    # label groups of sample that belong to the same peak\n",
    "    peak_groups, num_groups = sci_label(np.diff(p) < threshold)\n",
    " \n",
    "    # iterate through groups and take the mean as peak index\n",
    "    for i in np.unique(peak_groups)[1:]:\n",
    "    #for i in np.unique(peak_groups):    \n",
    "        peak_group = p[np.where(peak_groups == i)]\n",
    "        output = np.append(output, np.median(peak_group))\n",
    "    return output\n",
    "\n",
    "\"\"\"TIME DOMAIN\"\"\"\n",
    "#independent function to calculate RMSSD\n",
    "def calc_rmssd(list):\n",
    "    diff_nni = np.diff(list)#successive differences\n",
    "    return np.sqrt(np.mean(diff_nni ** 2))\n",
    "    \n",
    "    \n",
    " #independent function to calculate AVRR   \n",
    "def calc_avrr(list):\n",
    "    return sum(list)/len(list)\n",
    "\n",
    " #independent function to calculate SDRR   \n",
    "def calc_sdrr(list):\n",
    "    return statistics.stdev(list)\n",
    "\n",
    " #independent function to calculate SKEW   \n",
    "def calc_skew(list):\n",
    "    return skew(list)\n",
    "\n",
    " #independent function to calculate KURT   \n",
    "def calc_kurt(list):\n",
    "    return kurtosis(list)\n",
    "\n",
    "def calc_NNx(list):\n",
    "    #diff_nni = np.diff(list)\n",
    "    # detect peaks\n",
    "    peaks, similarity = detect_peaks(list, threshold=0.3)\n",
    "    # group peaks so we get a single peak per beat (hopefully)\n",
    "    grouped_peaks = group_peaks(peaks)\n",
    "    # RR-intervals are the differences between successive peaks\n",
    "    rr = np.diff(grouped_peaks)\n",
    "    nnxx = np.sum(np.abs(np.diff(rr)) > 50)*1\n",
    "    #return sum(np.abs(diff_nni) > 50)\n",
    "    return nnxx\n",
    "    \n",
    "def calc_pNNx(list):\n",
    "    #length_int = len(list)\n",
    "    #diff_nni = np.diff(list)\n",
    "    #nni_50 = sum(np.abs(diff_nni) > 50)\n",
    "    #return 100 * nni_50 / length_int\n",
    "    # detect peaks\n",
    "    peaks, similarity = detect_peaks(list, threshold=0.3)\n",
    "    # group peaks so we get a single peak per beat (hopefully)\n",
    "    grouped_peaks = group_peaks(peaks)\n",
    "    # RR-intervals are the differences between successive peaks\n",
    "    rr = np.diff(grouped_peaks)\n",
    "    pnnxx = 100 * np.sum((np.abs(np.diff(rr)) > 50)*1) / len(rr)\n",
    "    #return sum(np.abs(diff_nni) > 50)\n",
    "    return pnnxx\n",
    "    \n",
    "\"\"\"NON LINEAR DOMAIN\"\"\"\n",
    " #independent function to calculate SD1\n",
    "def calc_SD1(list):\n",
    "    diff_nn_intervals = np.diff(list)\n",
    "    return np.sqrt(np.std(diff_nn_intervals, ddof=1) ** 2 * 0.5)\n",
    " #independent function to calculate SD2\n",
    "def calc_SD2(list):\n",
    "    diff_nn_intervals = np.diff(list)\n",
    "    return np.sqrt(2 * np.std(list, ddof=1) ** 2 - 0.5 * np.std(\\\n",
    "                   diff_nn_intervals, ddof=1) ** 2)\n",
    "    \n",
    " #independent function to calculate SD1/SD2\n",
    "def calc_SD1overSD2(list):\n",
    "      diff_nn_intervals = np.diff(list)\n",
    "      sd1 = np.sqrt(np.std(diff_nn_intervals, ddof=1) ** 2 * 0.5)\n",
    "      sd2 = np.sqrt(2 * np.std(list, ddof=1) ** 2 - 0.5 * np.std(\\\n",
    "                    diff_nn_intervals, ddof=1) ** 2)\n",
    "      ratio_sd2_sd1 = sd2 / sd1\n",
    "      return ratio_sd2_sd1\n",
    "    \n",
    "    \n",
    " #independent function to calculate CSI\n",
    "def calc_CSI(list):\n",
    "    diff_nn_intervals = np.diff(list)\n",
    "    sd1 = np.sqrt(np.std(diff_nn_intervals, ddof=1) ** 2 * 0.5)\n",
    "    sd2 = np.sqrt(2 * np.std(list, ddof=1) ** 2 - 0.5 * np.std(\\\n",
    "                  diff_nn_intervals, ddof=1) ** 2)\n",
    "    L=4 * sd1\n",
    "    T=4 * sd2\n",
    "    return L/T\n",
    "       \n",
    " #independent function to calculate CVI\n",
    "def calc_CVI(list):\n",
    "    diff_nn_intervals = np.diff(list)\n",
    "    sd1 = np.sqrt(np.std(diff_nn_intervals, ddof=1) ** 2 * 0.5)\n",
    "    sd2 = np.sqrt(2 * np.std(list, ddof=1) ** 2 - 0.5 * np.std(\\\n",
    "                  diff_nn_intervals, ddof=1) ** 2)\n",
    "    L=4 * sd1\n",
    "    T=4 * sd2\n",
    "    return np.log10(L * T)\n",
    " \n",
    " #independent function to calculate modified CVI\n",
    "def calc_modifiedCVI(list):\n",
    "    diff_nn_intervals = np.diff(list)\n",
    "    sd1 = np.sqrt(np.std(diff_nn_intervals, ddof=1) ** 2 * 0.5)\n",
    "    sd2 = np.sqrt(2 * np.std(list, ddof=1) ** 2 - 0.5 * np.std(\\\n",
    "                  diff_nn_intervals, ddof=1) ** 2)\n",
    "    L=4 * sd1\n",
    "    T=4 * sd2\n",
    "    return L ** 2 / T\n",
    "\n",
    "\n",
    "def calc_meanrr(list):\n",
    "    # detect peaks\n",
    "    peaks, similarity = detect_peaks(list, threshold=0.3)\n",
    "    # group peaks so we get a single peak per beat (hopefully)\n",
    "    grouped_peaks = group_peaks(peaks)\n",
    "    # RR-intervals are the differences between successive peaks\n",
    "    rr = np.diff(grouped_peaks)\n",
    "    return np.mean(rr)\n",
    "\n",
    "def calc_medianrr(list):\n",
    "    # detect peaks\n",
    "    peaks, similarity = detect_peaks(list, threshold=0.3)\n",
    "    # group peaks so we get a single peak per beat (hopefully)\n",
    "    grouped_peaks = group_peaks(peaks)\n",
    "    # RR-intervals are the differences between successive peaks\n",
    "    rr = np.diff(grouped_peaks)\n",
    "    return np.median(rr)\n",
    "\n",
    "    \n",
    "def calc_hr(list):\n",
    "    # detect peaks\n",
    "    peaks, similarity = detect_peaks(list, threshold=0.3)\n",
    "    # group peaks so we get a single peak per beat (hopefully)\n",
    "    grouped_peaks = group_peaks(peaks)\n",
    "    # RR-intervals are the differences between successive peaks\n",
    "    rr = np.diff(grouped_peaks)\n",
    "    hr = 60000/rr\n",
    "    return np.mean(hr) \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_frame = train_d_frame.copy()\n",
    "#hrv_data = train_d_frame.copy()\n",
    "\n",
    "medianrr = []\n",
    "meanrr = []\n",
    "rmssd = []\n",
    "sdrr_rmssd = []\n",
    "hr = []\n",
    "avrr = []\n",
    "sdrr = []\n",
    "skew_ = []\n",
    "kurt = []\n",
    "NNx = []\n",
    "pNNx = []\n",
    "SD1 = []\n",
    "SD2 = []\n",
    "CSI = []\n",
    "CVI = []\n",
    "modifiedCVI = []\n",
    "SD1overSD2 = []\n",
    "for index, row in extract_frame.iterrows():\n",
    "    rmssd.append(calc_rmssd(row['feature']))\n",
    "    avrr.append(calc_avrr(row['feature']))\n",
    "    sdrr.append(calc_sdrr(row['feature']))\n",
    "    skew_.append(calc_skew(row['feature']))\n",
    "    kurt.append(calc_kurt(row['feature']))\n",
    "    NNx.append(calc_NNx(pd.Series(row['feature'])))\n",
    "    pNNx.append(calc_pNNx(pd.Series(row['feature'])))\n",
    "    SD1.append(calc_SD1(row['feature']))\n",
    "    SD2.append(calc_SD2(row['feature']))\n",
    "    CSI.append(calc_CSI(row['feature']))\n",
    "    CVI.append(calc_CVI(row['feature']))\n",
    "    modifiedCVI.append(calc_modifiedCVI(row['feature']))\n",
    "    SD1overSD2.append(calc_SD1overSD2(row['feature']))\n",
    "    meanrr.append(calc_meanrr(pd.Series(row['feature'])))\n",
    "    medianrr.append(calc_medianrr(pd.Series(row['feature'])))\n",
    "    hr.append(calc_hr(pd.Series(row['feature'])))\n",
    "    sdrrrmssd = calc_sdrr(row['feature']) / calc_rmssd(row['feature'])\n",
    "    sdrr_rmssd.append(sdrrrmssd)\n",
    "    \n",
    "\n",
    "extract_frame['meanrr'] = meanrr\n",
    "extract_frame['medianrr'] = medianrr\n",
    "extract_frame['sdrr'] = sdrr\n",
    "extract_frame['rmssd'] = rmssd\n",
    "extract_frame['sdrr_rmssd'] = sdrr_rmssd\n",
    "extract_frame['hr'] = hr\n",
    "extract_frame['NNx'] = NNx\n",
    "extract_frame['pNNx'] = pNNx\n",
    "extract_frame['sd1'] = SD1\n",
    "extract_frame['sd2'] = SD2\n",
    "extract_frame['avrr'] = avrr\n",
    "extract_frame['skew'] = skew_\n",
    "extract_frame['kurt'] = kurt\n",
    "extract_frame['avrr'] = avrr\n",
    "extract_frame['csi'] = CSI\n",
    "extract_frame['cvi'] = CVI\n",
    "extract_frame['modifiedcvi'] = modifiedCVI\n",
    "\n",
    "extract_frame = extract_frame.drop(['Valence level', 'Arousal level', 'Dominance level'], axis = 1)\n",
    "extract_frame2 = extract_frame.copy()\n",
    "extract_frame = extract_frame.drop(['feature', 'Target Emotion'], axis = 1)\n",
    "extract_frame = extract_frame.drop(['Valence', 'Arousal'], axis = 1)\n",
    "extract_frame = extract_frame.drop(['avrr', 'csi', 'cvi', 'modifiedcvi'], axis = 1)\n",
    "extract_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract_frame = extract_frame2.drop(['feature'], axis = 1)\n",
    "#extract_frame = extract_frame2.drop(['Target Emotion'], axis = 1)\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(extract_frame['emotion'])\n",
    "extract_frame['emotion'] = le.transform(extract_frame['emotion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,10))\n",
    "corr = extract_frame.corr()\n",
    "sns.heatmap(corr, annot=False, cmap=plt.cm.Reds)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection(correlation, threshold):\n",
    "    selected_features = []\n",
    "    for i in range(corr.shape[0]):\n",
    "      if corr.iloc[i,0] > threshold:\n",
    "        selected_features.append(extract_frame.iloc[:,i])\n",
    "    return pd.DataFrame(selected_features).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORRELATION_THRESHOLD = 0.1\n",
    "extract_frame = feature_selection(extract_frame, CORRELATION_THRESHOLD)\n",
    "x = extract_frame.drop(['emotion'],axis=1)\n",
    "y = extract_frame['emotion']\n",
    "train_x, test_x, train_y, test_y = train_test_split(x, y, train_size=0.8, test_size=0.2, random_state=0)\n",
    "\n",
    "extract_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig, axarr = plt.subplots(2, 2, figsize=(15, 9))\n",
    "#processed_frame['rmssd'][:70].plot(ax=axarr[0][0])\n",
    "#processed_frame['sdrr'][:70].plot(ax=axarr[1][0])\n",
    "#processed_frame['skew'][:70].plot(ax=axarr[0][1])\n",
    "#processed_frame['kurt'][:70].plot(ax=axarr[1][1])\n",
    "#axarr[0][0].set_title(\"rmssd\")\n",
    "#axarr[0][1].set_title(\"sdrr\")\n",
    "#axarr[1][0].set_title(\"skew\")\n",
    "#axarr[1][1].set_title(\"kurt\")\n",
    "#plt.subplots_adjust(hspace=.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "dt_model = DecisionTreeClassifier(criterion=\"entropy\", max_depth=12)\n",
    "dt_model = dt_model.fit(train_x, train_y)\n",
    "dt_pred_y = dt_model.predict(test_x)\n",
    "\n",
    "rf_model = RandomForestClassifier()\n",
    "rf_model.fit(train_x, train_y)\n",
    "rfpred_y = rf_model.predict(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decision Tree**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(test_y,dt_pred_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(test_y,rfpred_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def get_plot_ranges(start=10, end=20, n=5):\n",
    "    '''\n",
    "    Make an iterator that divides into n or n+1 ranges. \n",
    "    - if end-start is divisible by steps, return n ranges\n",
    "    - if end-start is not divisible by steps, return n+1 ranges, where the last range is smaller and ends at n\n",
    "    \n",
    "    # Example:\n",
    "    >> list(get_plot_ranges())\n",
    "    >> [(0.0, 3.0), (3.0, 6.0), (6.0, 9.0)]\n",
    "\n",
    "    '''\n",
    "#    distance = end - start\n",
    "#    for i in np.arange(start, end, np.floor(distance/n)):\n",
    "#        yield (int(i), int(np.minimum(end, np.floor(distance/n) + i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sampfrom = 0\n",
    "#sampto = 5000\n",
    "#nr_plots = 1\n",
    "\n",
    "#for start, stop in get_plot_ranges(sampfrom, sampto, nr_plots):\n",
    "    # get slice data of ECG data\n",
    "    #print(start)\n",
    "    #print(stop)\n",
    "#    cond_slice = (extract_frame.index >= start) & (extract_frame.index < stop) \n",
    "#    ecg_slice = extract_frame.heartrate[cond_slice]\n",
    "\n",
    "    # detect peaks\n",
    "#    peaks, similarity = detect_peaks(ecg_slice, threshold=0.3)\n",
    "    \n",
    "    # plot similarity\n",
    "#    plt.figure(figsize=(20, 15))\n",
    "\n",
    "#    plt.subplot(211)\n",
    "#    plt.title(\"ECG signal with found peaks\")\n",
    "#    plt.plot(ecg_slice.index, ecg_slice, label=\"ECG\", color=\"#51A6D8\", linewidth=1)\n",
    "#    plt.plot(peaks, np.repeat(600, peaks.shape[0]), label=\"peaks\", color=\"orange\", marker=\"o\", linestyle=\"None\")\n",
    "#    plt.legend(loc=\"upper right\")\n",
    "#    plt.xlabel(\"Time (milliseconds)\")\n",
    "#    plt.ylabel(\"Amplitude (arbitrary unit)\")\n",
    "    \n",
    " #   plt.subplot(212)\n",
    " #   plt.title('Similarity with QRS template')\n",
    " #   plt.plot(ecg_slice.index, similarity, label=\"Similarity with QRS filter\", color=\"olive\", linewidth=1)\n",
    " #   plt.legend(loc=\"upper right\")\n",
    " #   plt.xlabel(\"Time (milliseconds)\")\n",
    " #   plt.ylabel(\"Similarity (normalized)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect peaks\n",
    "#peaks, similarity = detect_peaks(df.heartrate, threshold=0.3)\n",
    "\n",
    "# group peaks\n",
    "#grouped_peaks = group_peaks(peaks)\n",
    "\n",
    "# plot peaks\n",
    "#plt.figure(figsize=(20, 7))\n",
    "#plt.title(\"Group similar peaks together\")\n",
    "#plt.plot(df.index, df.heartrate, label=\"ECG\", color=\"#51A6D8\", linewidth=2)\n",
    "#plt.plot(peaks, np.repeat(600, peaks.shape[0]),label=\"samples above threshold (found peaks)\", color=\"orange\", marker=\"o\", linestyle=\"None\")\n",
    "#plt.plot(grouped_peaks, np.repeat(620, grouped_peaks.shape[0]), label=\"median of found peaks\", color=\"k\", marker=\"v\", linestyle=\"None\")\n",
    "#plt.legend(loc=\"upper right\")\n",
    "#plt.xlabel(\"Time (ms)\")\n",
    "#plt.ylabel(\"Amplitude (arbitrary unit)\")\n",
    "#plt.gca().set_xlim(0, 200)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect peaks\n",
    "#peaks, similarity = detect_peaks(df.heartrate, threshold=0.3)\n",
    "\n",
    "# group peaks so we get a single peak per beat (hopefully)\n",
    "#grouped_peaks = group_peaks(peaks)\n",
    "\n",
    "# RR-intervals are the differences between successive peaks\n",
    "#rr = np.diff(grouped_peaks)\n",
    "\n",
    "# plot RR-intervals\n",
    "#plt.figure(figsize=(20, 7))\n",
    "#plt.title(\"RR-intervals\")\n",
    "#plt.xlabel(\"Time (ms)\")\n",
    "#plt.ylabel(\"RR-interval (ms)\")\n",
    "\n",
    "#plt.plot(np.cumsum(rr), rr, label=\"RR-interval\", color=\"#A651D8\")\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(20, 7))\n",
    "#plt.title(\"Distribution of RR-intervals\")\n",
    "\n",
    "#with warnings.catch_warnings():\n",
    "#    warnings.simplefilter(\"ignore\") # ignore FutureWarning \n",
    "#    sns.kdeplot(rr, label=\"rr-intervals\", color=\"#A651D8\", shade=True)\n",
    "\n",
    "#outlier_low = np.mean(rr) - 2 * np.std(rr)\n",
    "#outlier_high = np.mean(rr) + 2 * np.std(rr)\n",
    "\n",
    "#plt.axvline(x=outlier_low)\n",
    "#plt.axvline(x=outlier_high, label=\"outlier boundary\")\n",
    "#plt.text(outlier_low - 370, 0.004, \"outliers low (< mean - 2 sigma)\")\n",
    "#plt.text(outlier_high + 20, 0.004, \"outliers high (> mean + 2 sigma)\")\n",
    "\n",
    "#plt.xlabel(\"RR-interval (ms)\")\n",
    "#plt.ylabel(\"Density\")\n",
    "\n",
    "#plt.legend()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(20, 7))\n",
    "\n",
    "#rr_corrected = rr.copy()\n",
    "#rr_corrected[np.abs(zscore(rr)) > 2] = np.median(rr)\n",
    "\n",
    "#plt.title(\"RR-intervals\")\n",
    "#plt.xlabel(\"Time (ms)\")\n",
    "#plt.ylabel(\"RR-interval (ms)\")\n",
    "\n",
    "#plt.plot(rr, color=\"red\", label=\"RR-intervals\")\n",
    "#plt.plot(rr_corrected, color=\"green\",  label=\"RR-intervals after correction\")\n",
    "#plt.legend()\n",
    "#plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
